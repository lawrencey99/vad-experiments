{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['roots', 'copy']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import spatial\n",
    "\n",
    "def set_seed(seed = 1337):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed()\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\naudio_folder = \\'/home/lawrence/workspace/vad-experiments/dataset/LibriSpeech/train-clean-100/\\'\\n\\nX = []\\nY = []\\n\\n\\n        start = time.time()\\n\\n        adjust_learning_rate(optimizeross_entropy(X_batch, Y_batch)\\n---> 15         loss.backward()\\n     16         optimizer.step()\\nspeakers = os.listdir(audio_folder)\\nspeaker2id = {}\\nflac_files = []\\nfor speaker_id, speaker in enumerate(speakers):\\n    speaker2id[speaker] = speaker_id\\n    \\nfor root, dirs, files in os.walk(\"/home/lawrence/workspace/vad-experiments/dataset/LibriSpeech/train-clean-100\"):\\n    for file in files:\\n        if file.endswith(\".flac\"):\\n             flac_files.append(os.path.join(root, file))\\n\\nfor file_num, audio_file in enumerate(flac_files):\\n    speaker = os.path.basename(os.path.dirname(os.path.dirname(audio_file)))\\n    print(speaker)\\n    print(audio_file)\\n    y, sr = librosa.load(audio_file)\\n    print(\"asdf\")\\n    num_samples = y.shape[0]\\n    int_seconds = num_samples//sr\\n    y = y[:int_seconds * sr]\\n    samples = np.array(np.split(y, int_seconds))\\n    mfccs = []\\n    for i in range(len(samples)):\\n        X.append(np.mean(librosa.feature.mfcc(y=samples[i], sr=sr, n_mfcc=40).T,axis=0))\\n    Y += [speaker2id[speaker]] * len(samples)\\n    print(file_num)\\nnewX = np.array(X)\\nnewY = np.array(Y).reshape(-1, 1)\\nnp.save(\\'Xvals_test.np\\', newX)\\nnp.save(\\'Yvals_test.np\\', newY)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "audio_folder = '/home/lawrence/workspace/vad-experiments/dataset/LibriSpeech/train-clean-100/'\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        adjust_learning_rate(optimizeross_entropy(X_batch, Y_batch)\n",
    "---> 15         loss.backward()\n",
    "     16         optimizer.step()\n",
    "speakers = os.listdir(audio_folder)\n",
    "speaker2id = {}\n",
    "flac_files = []\n",
    "for speaker_id, speaker in enumerate(speakers):\n",
    "    speaker2id[speaker] = speaker_id\n",
    "    \n",
    "for root, dirs, files in os.walk(\"/home/lawrence/workspace/vad-experiments/dataset/LibriSpeech/train-clean-100\"):\n",
    "    for file in files:\n",
    "        if file.endswith(\".flac\"):\n",
    "             flac_files.append(os.path.join(root, file))\n",
    "\n",
    "for file_num, audio_file in enumerate(flac_files):\n",
    "    speaker = os.path.basename(os.path.dirname(os.path.dirname(audio_file)))\n",
    "    print(speaker)\n",
    "    print(audio_file)\n",
    "    y, sr = librosa.load(audio_file)\n",
    "    print(\"asdf\")\n",
    "    num_samples = y.shape[0]\n",
    "    int_seconds = num_samples//sr\n",
    "    y = y[:int_seconds * sr]\n",
    "    samples = np.array(np.split(y, int_seconds))\n",
    "    mfccs = []\n",
    "    for i in range(len(samples)):\n",
    "        X.append(np.mean(librosa.feature.mfcc(y=samples[i], sr=sr, n_mfcc=40).T,axis=0))\n",
    "    Y += [speaker2id[speaker]] * len(samples)\n",
    "    print(file_num)\n",
    "newX = np.array(X)\n",
    "newY = np.array(Y).reshape(-1, 1)\n",
    "np.save('Xvals_test.np', newX)\n",
    "np.save('Yvals_test.np', newY)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  (327745, 40)\n",
      "Test size:  (10000, 40)\n",
      "Val size:  (10000, 40)\n"
     ]
    }
   ],
   "source": [
    "X = np.load('Xvals.np.npy')\n",
    "Y = np.load('Yvals.np.npy').astype(numpy.int32)\n",
    "\n",
    "def split_train_test_val(X, Y, test_size=10000, val_size=10000):\n",
    "    train_test_size = test_size + val_size\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=train_test_size)\n",
    "    X_test, X_val, Y_test, Y_val = train_test_split(X_test, Y_test, test_size=val_size)\n",
    "    return X_train, X_test, X_val, np.squeeze(Y_train), np.squeeze(Y_test), np.squeeze(Y_val)\n",
    "\n",
    "X_train, X_test, X_val, Y_train, Y_test, Y_val = split_train_test_val(X, Y)\n",
    "print(\"Train size: \", X_train.shape)\n",
    "print(\"Test size: \", X_test.shape)\n",
    "print(\"Val size: \", X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VADDNN(nn.Module):\n",
    "    def __init__(self, input_size=40, output_size=496, dropout=0.5):\n",
    "        super(VADDNN, self).__init__()\n",
    "        self.linear_1 = nn.Linear(input_size, 256)\n",
    "        self.linear_2 = nn.Linear(256, 256)\n",
    "        self.linear_3 = nn.Linear(256, 256)\n",
    "        self.linear_4 = nn.Linear(256, 256)\n",
    "        self.linear_5 = nn.Linear(256, output_size)\n",
    "        self.dropout_1 = nn.Dropout(p=dropout)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = {}\n",
    "        out['h1'] = nn.functional.relu(self.linear_1(x))\n",
    "        out['h2'] = nn.functional.relu(self.linear_2(out['h1']))\n",
    "        out['h3'] = nn.functional.relu(self.linear_3(self.dropout_1(out['h2'])))\n",
    "        out['h4'] = nn.functional.relu(self.linear_4(self.dropout_2(out['h3'])))\n",
    "        out['out'] = nn.functional.relu(self.linear_5(out['h4']))\n",
    "        return [out['h4'], out['out']]\n",
    "    \n",
    "class VADCNN(nn.Module):\n",
    "    def __init__(self, input_size=40, output_size=496, dropout=0.5):\n",
    "        super(VADDNN, self).__init__()\n",
    "        self.linear_1 = nn.Linear(input_size, 256)\n",
    "        self.linear_2 = nn.Linear(256, 256)\n",
    "        self.linear_3 = nn.Linear(256, 256)\n",
    "        self.linear_4 = nn.Linear(256, 256)\n",
    "        self.linear_5 = nn.Linear(256, output_size)\n",
    "        self.dropout_1 = nn.Dropout(p=dropout)\n",
    "        self.dropout_2 = nn.Dropout(p=dropout)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = {}\n",
    "        out['h1'] = nn.functional.relu(self.linear_1(x))\n",
    "        out['h2'] = nn.functional.relu(self.linear_2(out['h1']))\n",
    "        out['h3'] = nn.functional.relu(self.linear_3(self.dropout_1(out['h2'])))\n",
    "        out['h4'] = nn.functional.relu(self.linear_4(self.dropout_2(out['h3'])))\n",
    "        out['out'] = nn.functional.relu(self.linear_5(out['h4']))\n",
    "        return [out['h4'], out['out']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "train_dataset = data.TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(Y_train))\n",
    "dataloader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "def get_model_name(epoch):\n",
    "    return 'model/VAD_DNN_epoch_{}.pkl'.format(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "START_EPOCH = 100\n",
    "if START_EPOCH == 0:    \n",
    "    model = VADDNN()\n",
    "    model.cuda()\n",
    "else:\n",
    "    model = torch.load(get_model_name(START_EPOCH))\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3 / 1000, momentum=0.9, weight_decay=1e-6)\n",
    "for epoch in range(START_EPOCH + 1, EPOCHS + 1):\n",
    "    total_loss = 0.0\n",
    "    epoch_correct = 0\n",
    "    epoch_size = 0\n",
    "    for batch_id, (X_batch, Y_batch) in enumerate(dataloader):\n",
    "        X_batch, Y_batch = autograd.Variable(X_batch.cuda()), autograd.Variable(Y_batch.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        _, output = model(X_batch)\n",
    "        loss = functional.cross_entropy(output, Y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "        \n",
    "        pred = output.data.max(1)[1]\n",
    "        epoch_correct += pred.eq(Y_batch.data.view_as(pred)).sum()\n",
    "        epoch_size += output.size()[0]\n",
    "    acc = 100.0 * epoch_correct / epoch_size\n",
    "    print('Train Epoch: {}\\t'\n",
    "          'Loss: {:.6f}\\t'\n",
    "          'Train Acc: {}/{} = {:.4f}%\\t'\n",
    "          'Training Time: {}'.format(\n",
    "            epoch,\n",
    "            total_loss,\n",
    "            epoch_correct,\n",
    "            epoch_size,\n",
    "            acc,\n",
    "            time.time() - start_time))\n",
    "    if epoch > len(epoch_loss):\n",
    "        epoch_loss.append(total_loss)\n",
    "    else:\n",
    "        epoch_loss[epoch-1] = total_loss\n",
    "    model_name = get_model_name(epoch)\n",
    "    torch.save(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Acc: 293544/327745 = 89.5648%\t\n"
     ]
    }
   ],
   "source": [
    "### Because we use dropout, train accuracy in eval mode may differ significantly from train accuracy in train mode\n",
    "\n",
    "load_model_name = 'model/VAD_DNN_epoch_100.pkl'\n",
    "model = torch.load(load_model_name)\n",
    "model.eval()\n",
    "BATCH_SIZE = 256\n",
    "train_dataset = data.TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(Y_train))\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "num_correct = 0\n",
    "num_total = 0\n",
    "for batch_id, (X_batch, Y_batch) in enumerate(train_dataloader):\n",
    "    X_batch, Y_batch = autograd.Variable(X_batch.cuda()), autograd.Variable(Y_batch.cuda())\n",
    "    _, output = model(X_batch)\n",
    "    pred = output.data.max(1)[1]\n",
    "    num_correct += pred.eq(Y_batch.data.view_as(pred)).sum()\n",
    "    num_total += output.size()[0]\n",
    "acc = 100.0 * num_correct / num_total\n",
    "print('Train Acc: {}/{} = {:.4f}%\\t'.format(\n",
    "        num_correct,\n",
    "        num_total,\n",
    "        acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 8961/10000 = 89.6100%\t\n"
     ]
    }
   ],
   "source": [
    "load_model_name = 'model/VAD_DNN_epoch_100.pkl'\n",
    "model = torch.load(load_model_name)\n",
    "model.eval()\n",
    "BATCH_SIZE = 256\n",
    "test_dataset = data.TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(Y_test))\n",
    "test_dataloader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "num_correct = 0\n",
    "num_total = 0\n",
    "for batch_id, (X_batch, Y_batch) in enumerate(test_dataloader):\n",
    "    X_batch, Y_batch = autograd.Variable(X_batch.cuda()), autograd.Variable(Y_batch.cuda())\n",
    "    _, output = model(X_batch)\n",
    "    pred = output.data.max(1)[1]\n",
    "    num_correct += pred.eq(Y_batch.data.view_as(pred)).sum()\n",
    "    num_total += output.size()[0]\n",
    "acc = 100.0 * num_correct / num_total\n",
    "print('Test Acc: {}/{} = {:.4f}%\\t'.format(\n",
    "        num_correct,\n",
    "        num_total,\n",
    "        acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lawrence-13.m4a', 'lawrence-9.m4a', 'lawrence-5.m4a', 'lawrence-11.m4a', 'lawrence-1.m4a', 'lawrence-3.m4a', 'lawrence-6.m4a', 'lawrence-4.m4a', 'lawrence-2.m4a', 'lawrence-14.m4a', 'lawrence-15.m4a', 'lawrence-8.m4a', 'lawrence-12.m4a', 'lawrence-7.m4a', 'lawrence-10.m4a']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 40)\n"
     ]
    }
   ],
   "source": [
    "voice_sample_dir = 'dataset/text-dependent/'\n",
    "lawrence_sample_files = []\n",
    "for roots, dirs, files in os.walk(voice_sample_dir):\n",
    "    for file in files:\n",
    "        if file.startswith('lawrence') and file.endswith('m4a'):\n",
    "            lawrence_sample_files.append(file)\n",
    "print(lawrence_sample_files)\n",
    "loaded_files = []\n",
    "for filename in lawrence_sample_files:\n",
    "    filename = os.path.join(voice_sample_dir, filename)\n",
    "    y, sr = librosa.load(filename)\n",
    "    loaded_files.append(np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0))\n",
    "loaded_files = np.array(loaded_files)\n",
    "print(loaded_files.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 40)\n"
     ]
    }
   ],
   "source": [
    "loaded_files = np.array(loaded_files)\n",
    "print(loaded_files.shape)\n",
    "\n",
    "enrollment_set = loaded_files[:12]\n",
    "utterance_set = loaded_files[12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VADDNN(\n",
       "  (linear_1): Linear(in_features=40, out_features=256, bias=True)\n",
       "  (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (linear_3): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (linear_4): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (linear_5): Linear(in_features=256, out_features=496, bias=True)\n",
       "  (dropout_1): Dropout(p=0.5, inplace=False)\n",
       "  (dropout_2): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model_name = 'model/VAD_DNN_epoch_100.pkl'\n",
    "model = torch.load(load_model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.TensorDataset(torch.FloatTensor(enrollment_set))\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=12, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for batch_id, (X_enrollment,) in enumerate(train_dataloader):\n",
    "        X_enrollment = X_enrollment.cuda()\n",
    "        enrollment_output, output = model(X_enrollment)\n",
    "enrollment_output = enrollment_output.cpu().numpy()\n",
    "enrollment_embedding = np.mean(enrollment_output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n",
      "/home/lawrence/workspace/pytorchenv/lib/python3.6/site-packages/librosa/core/audio.py:146: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  warnings.warn('PySoundFile failed. Trying audioread instead.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 40)\n",
      "(13, 40)\n"
     ]
    }
   ],
   "source": [
    "voice_sample_dir = 'dataset/text-dependent/'\n",
    "josh_sample_files = []\n",
    "for roots, dirs, files in os.walk(voice_sample_dir):\n",
    "    for file in files:\n",
    "        if file.startswith('josh') and file.endswith('m4a'):\n",
    "            josh_sample_files.append(file)\n",
    "josh_files = []\n",
    "for filename in josh_sample_files:\n",
    "    filename = os.path.join(voice_sample_dir, filename)\n",
    "    y, sr = librosa.load(filename)\n",
    "    josh_files.append(np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T,axis=0))\n",
    "josh_files = np.array(josh_files)\n",
    "print(josh_files.shape)\n",
    "\n",
    "utterance_test_set = np.vstack([josh_files, utterance_set])\n",
    "print(utterance_test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data.TensorDataset(torch.FloatTensor(utterance_test_set))\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=13, shuffle=False)\n",
    "with torch.no_grad():\n",
    "    for batch_id, (X_enrollment,) in enumerate(train_dataloader):\n",
    "        X_enrollment = X_enrollment.cuda()\n",
    "        enrollment_output, output = model(X_enrollment)\n",
    "test_output = enrollment_output.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.542151153087616, 0.396335244178772, 0.43813949823379517, 0.4419863820075989, 0.6182933747768402, 0.505691260099411, 0.47575628757476807, 0.4808054566383362, 0.4512820839881897, 0.3462369441986084, 0.053267836570739746, 0.11287206411361694, 0.05464053153991699]\n",
      "Minimum distance for josh utterances vs lawrence embedding is: 0.3462369441986084\n",
      "Maximum distance for lawrence utterance vs lawrence embedding is: 0.11287206411361694\n",
      "\n",
      "Based on this, we can probably use experimental threshold of 0.20 and still have decent margin for error\n"
     ]
    }
   ],
   "source": [
    "cos_distance = []\n",
    "for utterance in test_output:\n",
    "    cos_distance.append(spatial.distance.cosine(enrollment_embedding, utterance))\n",
    "print(cos_distance)\n",
    "print(\"Minimum distance for josh utterances vs lawrence embedding is:\", np.min(cos_distance[:10]))\n",
    "print(\"Maximum distance for lawrence utterance vs lawrence embedding is:\", np.max(cos_distance[10:]))\n",
    "print(\"\")\n",
    "print(\"Based on this, we can probably use experimental threshold of 0.20 and still have decent margin for error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
